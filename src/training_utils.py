# src/training_utils.py

from typing import Dict, Any
from sklearn.base import BaseEstimator
from src.model_registry import get_model as get_task_model
from src.mlflow_utils import log_with_mlflow
from src.tasks.registry import get_task_evaluator
from src.factories.spec_builder import build_from_spec


def train_eval_log(
    model_name: str,
    model_params: Dict,
    X_train,
    y_train,
    X_test,
    y_test,
    pre=None,
    mlflow_experiment: str | None = None,
    col_manager=None,
    model_mlflow_cfg: dict | None = None,
    task: str = "regression",
) -> Dict[str, Any]:
    """
    Generic training routine for sklearn-compatible models.
    Performs:
      - model construction (via registry)
      - fit() on train data
      - predict() on test data
      - MSE + R¬≤ evaluation
      - MLflow logging (with optional preprocessor pipeline)
      - task : str
        The type of ML task (e.g. "regression", "classification").
        This controls which metrics are computed.

    Returns:
      metrics: Dict[str, float]
    """

    # ------------------------------------------------------------
    # 1Ô∏è‚É£ BUILD MODEL
    # ------------------------------------------------------------
    # model : BaseEstimator = get_task_model(task, model_name, **(model_params or {}))
    
    # Pattern A: spec-driven construction
    # model_name must exist in ESTIMATOR_CATALOG (as a "type" key)
    spec = {
        "type": model_name,
        "params": (model_params or {}),
    }
    model: BaseEstimator = build_from_spec(spec)

    # Capability check
    has_fit     = hasattr(model, "fit")
    has_predict = hasattr(model, "predict")

    print(f"\nüîç Model: {model_name}")
    print(f"   ‚Üí fit():     {'YES' if has_fit else 'NO'}")
    print(f"   ‚Üí predict(): {'YES' if has_predict else 'NO'}")

    if not (has_fit and has_predict):
        raise TypeError(
            f"‚ùå Model '{model_name}' does NOT implement sklearn API "
            f"(fit={has_fit}, predict={has_predict})"
        )

    # ------------------------------------------------------------
    # 2Ô∏è‚É£ TRAIN
    # ------------------------------------------------------------
    model.fit(X_train, y_train)

    # ------------------------------------------------------------
    # 3Ô∏è‚É£ Task-specific prediction & metrics
    # ------------------------------------------------------------
    evaluator = get_task_evaluator(task)

    y_pred, extra_pred_outputs = evaluator.predict(model, X_test)
    metrics = evaluator.evaluate(y_test, y_pred, extra_pred_outputs)

    # ------------------------------------------------------------
    # 4Ô∏è‚É£ PRINT COEFFICIENTS IF AVAILABLE
    # ------------------------------------------------------------
    print("üìä Metrics:")
    for k, v in metrics.items():
        print(f"   {k} = {v:.6f}" if isinstance(v, float) else f"   {k} = {v}")

    # ------------------------------------------------------------
    # 5Ô∏è‚É£ PRINT COEFFICIENTS IF AVAILABLE
    # ------------------------------------------------------------
    has_coef = hasattr(model, "coef_")
    has_importance = hasattr(model, "feature_importances_")

    if has_coef:
        print("\nüß† Learned Coefficients:")
        print(model.coef_)

    elif has_importance:
        print("\nüå≤ Feature Importances:")
        print(model.feature_importances_)

    else:
        print("\n‚ö†Ô∏è Model does not expose coefficients or feature_importances_")

    # ------------------------------------------------------------
    # 6Ô∏è‚É£ LOG TO MLFLOW
    # ------------------------------------------------------------
    log_with_mlflow(
        run_name=model_name,
        model=model,
        params=model_params or {},
        metrics=metrics,
        pre=pre,
        experiment=mlflow_experiment,
        col_manager=col_manager,
        model_mlflow_cfg=model_mlflow_cfg,
    )

    return {
        "metrics": metrics
    }
